{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "55546e2ea1c04f258987488a88eab593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d81f30153ae44c0b7d44c6d123ddff7",
              "IPY_MODEL_2022c848e5de4dbcb7075cc21afe9453",
              "IPY_MODEL_079ca7ce517b4a4fb415b24f65e52ea6"
            ],
            "layout": "IPY_MODEL_b81fc43f088d45a599699b536429dffd"
          }
        },
        "0d81f30153ae44c0b7d44c6d123ddff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aeb3038e6c1417bbdf35f4890f7108a",
            "placeholder": "​",
            "style": "IPY_MODEL_96f1957fffea419d89d2479f2453e476",
            "value": "Map: 100%"
          }
        },
        "2022c848e5de4dbcb7075cc21afe9453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0607082b1824e118ecbb5c4e0601b9b",
            "max": 76,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cf625db519d4e148c065b5a79dc417c",
            "value": 76
          }
        },
        "079ca7ce517b4a4fb415b24f65e52ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e878f44f05de4122bcf9134aa853a1ac",
            "placeholder": "​",
            "style": "IPY_MODEL_f0297824c6194a7f92bb9d87f067033f",
            "value": " 76/76 [00:00&lt;00:00, 917.40 examples/s]"
          }
        },
        "b81fc43f088d45a599699b536429dffd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aeb3038e6c1417bbdf35f4890f7108a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96f1957fffea419d89d2479f2453e476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0607082b1824e118ecbb5c4e0601b9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cf625db519d4e148c065b5a79dc417c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e878f44f05de4122bcf9134aa853a1ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0297824c6194a7f92bb9d87f067033f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/\n"
      ],
      "metadata": {
        "id": "GtoXAJjBY1fX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q  torch peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 accelerate\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "n5kH6OUjdubg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "#data_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "#training_data = load_dataset(data_name, split=\"train\")"
      ],
      "metadata": {
        "id": "xlrhh7QIJqtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/train.json') as f_in:\n",
        "  train= json.load(f_in)\n",
        "\n",
        "for i in range(len(train)):\n",
        "  train[i]['text'] = train[i]['text'].replace('Given the question delimited by triple backticks ```{','<s>[INST] ').replace('}```, what is the answer? Answer: {',' [/INST] ').replace('}',' </s>')\n",
        "\n",
        "with open('/content/train_llama.json', 'w') as outfile:\n",
        "    json.dump(train, outfile, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "5zHnDzDZnQUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and tokenizer names\n",
        "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "refined_model = \"llama-2-7b-mlabonne-enhanced\" #You can give it your own name"
      ],
      "metadata": {
        "id": "2mhSqk4dFlCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n"
      ],
      "metadata": {
        "id": "p_4sfVcqFmuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = load_dataset(\"json\", data_files=\"/content/train_llama.json\",split=\"train\")"
      ],
      "metadata": {
        "id": "fAOJZLKEnlHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[0]"
      ],
      "metadata": {
        "id": "JkNqgKwhu75P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization Config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")"
      ],
      "metadata": {
        "id": "t8ctESIgFqQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4holnKMdtIv"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA Config\n",
        "peft_parameters = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=8,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Training Params\n",
        "train_params = TrainingArguments(\n",
        "    output_dir=\"./results_modified\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=25,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "fine_tuning = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=training_data,\n",
        "    peft_config=peft_parameters,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=llama_tokenizer,\n",
        "    args=train_params\n",
        ")\n",
        "\n",
        "# Training\n",
        "fine_tuning.train()\n",
        "\n",
        "# Save Model\n",
        "fine_tuning.model.save_pretrained(refined_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711,
          "referenced_widgets": [
            "55546e2ea1c04f258987488a88eab593",
            "0d81f30153ae44c0b7d44c6d123ddff7",
            "2022c848e5de4dbcb7075cc21afe9453",
            "079ca7ce517b4a4fb415b24f65e52ea6",
            "b81fc43f088d45a599699b536429dffd",
            "3aeb3038e6c1417bbdf35f4890f7108a",
            "96f1957fffea419d89d2479f2453e476",
            "f0607082b1824e118ecbb5c4e0601b9b",
            "2cf625db519d4e148c065b5a79dc417c",
            "e878f44f05de4122bcf9134aa853a1ac",
            "f0297824c6194a7f92bb9d87f067033f"
          ]
        },
        "id": "TSsPNqspeFt5",
        "outputId": "8b3e45d7-da90-480d-e6bf-bed8eca0b75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/76 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55546e2ea1c04f258987488a88eab593"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [190/190 05:22, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.051400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.158700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.856800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.584100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.370100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.246300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.184100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, refined_model)\n"
      ],
      "metadata": {
        "id": "x5yAtJ-OWa0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/test.json') as f_in:\n",
        "  test= json.load(f_in)\n",
        "\n",
        "for i in range(len(test)):\n",
        "  test[i]['text'] = test[i]['text'].replace('Given the question delimited by triple backticks ```{','<s>[INST] ').replace('}```, what is the answer? Answer: {',' [/INST] ').replace('}',' </s>')\n",
        "\n",
        "with open('/content/test_llama.json', 'w') as outfile:\n",
        "    json.dump(test, outfile, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "_5Sc1kjiTso4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = [x['text'][10:x['text'].index('?')+1] for x in test]"
      ],
      "metadata": {
        "id": "iKlu3RbNzoV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Text\n",
        "pred_llama = []\n",
        "i = 0\n",
        "for x in test:\n",
        "  print(i)\n",
        "  #query = \"what is an AI virtual assistant?\"\n",
        "  text_gen = pipeline(task=\"text-generation\", model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
        "  output = text_gen(f\"<s>[INST] {x} [/INST]\")\n",
        "  pred_llama.append(output[0]['generated_text'])\n",
        "  i+=1\n",
        "\n",
        "#print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "OtXBD2kreFmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_llama = [x[x.index('? [/INST]')+10:] for x in pred_llama]"
      ],
      "metadata": {
        "id": "NkC1kr8M3OO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/pred_llama.json', 'w') as outfile:\n",
        "    json.dump(pred_llama, outfile, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "UzbC1oLM8SG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/test_llama.json') as f_in:\n",
        "  test= json.load(f_in)"
      ],
      "metadata": {
        "id": "RU8sZKHeBMNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [x['text'][x['text'].index('? [/INST]')+10:-5] for x in test]"
      ],
      "metadata": {
        "id": "ZROBGdxdBNk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "nltk.download('punkt')\n",
        "def bleu_score(ref,pred):\n",
        "\n",
        "  pred_tokens = nltk.word_tokenize(pred.lower())\n",
        "  ref_tokens = nltk.word_tokenize(ref.lower())\n",
        "\n",
        "  # Calculate BLEU score\n",
        "  bleu_score = sentence_bleu(ref_tokens, pred_tokens,smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "  return bleu_score\n",
        "\n",
        "score= []\n",
        "for i in range(len(test)):\n",
        "  score.append(bleu_score(test[i],pred_llama[i]))\n",
        "\n",
        "print(np.mean(score))"
      ],
      "metadata": {
        "id": "k0BeZvVM-YiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f9e2d2-913f-453e-aba1-79e6cadf7cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001766395086111293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/pred2.json') as f_in:\n",
        "  pred= json.load(f_in)"
      ],
      "metadata": {
        "id": "AC9x43b39Bs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score= []\n",
        "for i in range(len(test)):\n",
        "  score.append(bleu_score(test[i],pred[i]))\n",
        "\n",
        "print(np.mean(score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwKUQp7eQLBf",
        "outputId": "950255d4-8f36-41b6-97c5-186284fe4101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00272857343512289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def cosine_distance(reference_answer,generated_answer ):\n",
        "\n",
        "  reference_tokens = nltk.word_tokenize(reference_answer.lower())\n",
        "  generated_tokens = nltk.word_tokenize(generated_answer.lower())\n",
        "\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  reference_tokens = [token for token in reference_tokens if token not in stop_words]\n",
        "  generated_tokens = [token for token in generated_tokens if token not in stop_words]\n",
        "\n",
        "\n",
        "  reference_text = ' '.join(reference_tokens)\n",
        "  generated_text = ' '.join(generated_tokens)\n",
        "\n",
        "\n",
        "  vectorizer = CountVectorizer().fit_transform([reference_text, generated_text])\n",
        "  vectors = vectorizer.toarray()\n",
        "\n",
        "\n",
        "  cosine_sim = cosine_similarity([vectors[0]], [vectors[1]])\n",
        "\n",
        "  return cosine_sim[0][0]\n",
        "\n",
        "cosine_results = []\n",
        "for i in range(len(test)):\n",
        "  cosine_results.append(cosine_distance(test[i],pred[i]))\n",
        "\n",
        "print(np.mean(cosine_results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiIZY2qpIXz_",
        "outputId": "7240bb94-9518-4283-d15a-f84ab55c58f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4711368925945207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_results = []\n",
        "for i in range(len(test)):\n",
        "  cosine_results.append(cosine_distance(test[i],pred_llama[i]))\n",
        "\n",
        "print(np.mean(cosine_results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5s2vETzQp20",
        "outputId": "fb7d69d8-b09f-4425-c892-42a9147421de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4366043670856687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_responses(question_num,question,test,pred):\n",
        "\n",
        "    # Retrieve the responses corresponding to the given question number\n",
        "    question = question[question_num - 1]\n",
        "    model_response = test[question_num - 1]\n",
        "    chatgpt_response = pred[question_num - 1]\n",
        "\n",
        "    screen_width = 120\n",
        "    print(textwrap.fill('Question:'+ question, width=screen_width)+ '\\n')\n",
        "    print(textwrap.fill(\"Model's Response:\"+ model_response , width=screen_width)+ '\\n')\n",
        "    print(textwrap.fill(\"ChatGPT's Response:\"+ chatgpt_response, width=screen_width))\n"
      ],
      "metadata": {
        "id": "A2BDI2x8SGZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "question_number = int(input(\"Enter the question number: \"))\n",
        "print_responses(question_number,question,test,pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8AhTUopWXK5",
        "outputId": "c8c44215-2554-4fc7-94b1-591ccbcb3a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the question number: 33\n",
            "Question:Can you explain how speech recognition systems are trained?\n",
            "\n",
            "Model's Response:Training a speech recognition system involves teaching the system to recognize patterns in speech data\n",
            "and associate them with corresponding textual transcriptions. The training process for speech recognition systems\n",
            "include: Data Collection, Data Preprocessing, Annotation, Model Selection, Model Training, Evaluation, Optimization and\n",
            "Tuning.\n",
            "\n",
            "ChatGPT's Response:Speech recognition systems, also known as speech-to-text or voice recognition systems, are trained\n",
            "using a large dataset of speech recordings with corresponding transcriptions or translations. They learn to recognize\n",
            "patterns in speech sounds, syntax, and grammar to convert spoken language into text. The training process typically\n",
            "involves the following steps: Data Collection, Data Preprocessing, Model Selection, Training, Evaluation, Fine-Tuning\n",
            "and Deployment. Data collection involves gathering a vast amount of speech recordings covering various languages,\n",
            "dialects, and topics. Data preprocessing involves cleaning, normalizing, and converting the recordings into a digital\n",
            "format suitable for model training. Model selection involves choosing a suitable speech recognition model, such as a\n",
            "neural network or statistical model. Training involves feeding the preprocessed data to the model and adjusting the\n",
            "model parameters to learn speech patterns. Evaluation involves\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8d5rSOccDXB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}